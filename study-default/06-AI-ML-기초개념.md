# ğŸ¤– AI/ML ê¸°ì´ˆ ê°œë…

## ğŸ“š ëª©ì°¨
1. [ì¸ê³µì§€ëŠ¥ ê°œìš”](#ì¸ê³µì§€ëŠ¥-ê°œìš”)
2. [ê¸°ê³„í•™ìŠµ ê¸°ì´ˆ](#ê¸°ê³„í•™ìŠµ-ê¸°ì´ˆ)
3. [ë”¥ëŸ¬ë‹ê³¼ ì‹ ê²½ë§](#ë”¥ëŸ¬ë‹ê³¼-ì‹ ê²½ë§)
4. [ìì—°ì–´ ì²˜ë¦¬ (NLP)](#ìì—°ì–´-ì²˜ë¦¬-nlp)
5. [ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ (LLM)](#ëŒ€ê·œëª¨-ì–¸ì–´-ëª¨ë¸-llm)

---

## ğŸ§  ì¸ê³µì§€ëŠ¥ ê°œìš”

### AIì˜ ë°œì „ ë‹¨ê³„
```mermaid
graph TB
    subgraph "AI ë°œì „ ì—­ì‚¬"
        AI[ì¸ê³µì§€ëŠ¥<br/>1950s~]
        ML[ê¸°ê³„í•™ìŠµ<br/>1980s~]
        DL[ë”¥ëŸ¬ë‹<br/>2010s~]
        LLM[ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸<br/>2020s~]
        
        AI --> ML
        ML --> DL
        DL --> LLM
    end
```

### AI vs ML vs DL
```mermaid
graph LR
    subgraph "í¬í•¨ ê´€ê³„"
        AI[AI<br/>ì¸ê³µì§€ëŠ¥]
        AI --> ML[ML<br/>ê¸°ê³„í•™ìŠµ]
        ML --> DL[DL<br/>ë”¥ëŸ¬ë‹]
        DL --> LLM[LLM<br/>ì–¸ì–´ëª¨ë¸]
    end
```

| êµ¬ë¶„ | ì„¤ëª… | ì˜ˆì‹œ |
|------|------|------|
| **AI** | ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°© | ì²´ìŠ¤ AI, ìŒì„± ì¸ì‹ |
| **ML** | ë°ì´í„°ë¡œë¶€í„° í•™ìŠµ | ìŠ¤íŒ¸ í•„í„°, ì¶”ì²œ ì‹œìŠ¤í…œ |
| **DL** | ì‹ ê²½ë§ì„ ì´ìš©í•œ í•™ìŠµ | ì´ë¯¸ì§€ ì¸ì‹, ë²ˆì—­ |
| **LLM** | ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ í•™ìŠµ | GPT, Claude, Mistral |

## ğŸ“Š ê¸°ê³„í•™ìŠµ ê¸°ì´ˆ

### í•™ìŠµ ìœ í˜•
```mermaid
graph TB
    subgraph "ê¸°ê³„í•™ìŠµ ìœ í˜•"
        SUP[ì§€ë„í•™ìŠµ<br/>Supervised<br/>ì •ë‹µ ìˆìŒ]
        UNSUP[ë¹„ì§€ë„í•™ìŠµ<br/>Unsupervised<br/>ì •ë‹µ ì—†ìŒ]
        REIN[ê°•í™”í•™ìŠµ<br/>Reinforcement<br/>ë³´ìƒ ê¸°ë°˜]
    end
```

### ì§€ë„í•™ìŠµ ì˜ˆì‹œ
```python
# ê°„ë‹¨í•œ ì„ í˜• íšŒê·€ ì˜ˆì‹œ
import numpy as np
from sklearn.linear_model import LinearRegression

# ë°ì´í„° ì¤€ë¹„ (ì§‘ í¬ê¸° -> ê°€ê²©)
X = np.array([[30], [50], [70], [100], [150]])  # í‰ìˆ˜
y = np.array([1, 2, 3, 4, 5])  # ê°€ê²© (ì–µì›)

# ëª¨ë¸ í•™ìŠµ
model = LinearRegression()
model.fit(X, y)

# ì˜ˆì¸¡
new_house = np.array([[80]])
predicted_price = model.predict(new_house)
print(f"80í‰ ì§‘ì˜ ì˜ˆìƒ ê°€ê²©: {predicted_price[0]:.2f}ì–µì›")

# ë¶„ë¥˜ ì˜ˆì‹œ (ìŠ¤íŒ¸ í•„í„°)
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer

# í›ˆë ¨ ë°ì´í„°
emails = [
    "ë¬´ë£Œ ì¿ í° ë°›ìœ¼ì„¸ìš”",
    "íšŒì˜ ì¼ì • í™•ì¸",
    "ë‹¹ì²¨ë˜ì…¨ìŠµë‹ˆë‹¤ í´ë¦­í•˜ì„¸ìš”",
    "í”„ë¡œì íŠ¸ ì§„í–‰ ìƒí™©"
]
labels = [1, 0, 1, 0]  # 1: ìŠ¤íŒ¸, 0: ì •ìƒ

# í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(emails)

# ëª¨ë¸ í•™ìŠµ
classifier = MultinomialNB()
classifier.fit(X, labels)

# ìƒˆ ì´ë©”ì¼ ë¶„ë¥˜
new_email = ["íŠ¹ë³„ í• ì¸ ì´ë²¤íŠ¸"]
new_X = vectorizer.transform(new_email)
prediction = classifier.predict(new_X)
print(f"ìŠ¤íŒ¸ ì—¬ë¶€: {'ìŠ¤íŒ¸' if prediction[0] == 1 else 'ì •ìƒ'}")
```

### ëª¨ë¸ í‰ê°€
```python
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ëª¨ë¸ í•™ìŠµ
model.fit(X_train, y_train)

# ì˜ˆì¸¡ ë° í‰ê°€
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ì •í™•ë„: {accuracy:.2%}")

# í˜¼ë™ í–‰ë ¬
cm = confusion_matrix(y_test, y_pred)
print("í˜¼ë™ í–‰ë ¬:")
print(cm)
```

## ğŸ”® ë”¥ëŸ¬ë‹ê³¼ ì‹ ê²½ë§

### ì‹ ê²½ë§ êµ¬ì¡°
```mermaid
graph LR
    subgraph "ì…ë ¥ì¸µ"
        I1((x1))
        I2((x2))
        I3((x3))
    end
    
    subgraph "ì€ë‹‰ì¸µ 1"
        H1((h1))
        H2((h2))
        H3((h3))
    end
    
    subgraph "ì€ë‹‰ì¸µ 2"
        H4((h4))
        H5((h5))
    end
    
    subgraph "ì¶œë ¥ì¸µ"
        O1((y))
    end
    
    I1 --> H1
    I1 --> H2
    I1 --> H3
    I2 --> H1
    I2 --> H2
    I2 --> H3
    I3 --> H1
    I3 --> H2
    I3 --> H3
    
    H1 --> H4
    H1 --> H5
    H2 --> H4
    H2 --> H5
    H3 --> H4
    H3 --> H5
    
    H4 --> O1
    H5 --> O1
```

### í™œì„±í™” í•¨ìˆ˜
```python
import numpy as np
import matplotlib.pyplot as plt

# ì£¼ìš” í™œì„±í™” í•¨ìˆ˜
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def relu(x):
    return np.maximum(0, x)

def tanh(x):
    return np.tanh(x)

def softmax(x):
    exp_x = np.exp(x - np.max(x))
    return exp_x / exp_x.sum()

# ì‹œê°í™”
x = np.linspace(-5, 5, 100)

plt.figure(figsize=(12, 3))

plt.subplot(1, 3, 1)
plt.plot(x, sigmoid(x))
plt.title('Sigmoid')

plt.subplot(1, 3, 2)
plt.plot(x, relu(x))
plt.title('ReLU')

plt.subplot(1, 3, 3)
plt.plot(x, tanh(x))
plt.title('Tanh')

plt.show()
```

### ê°„ë‹¨í•œ ì‹ ê²½ë§ êµ¬í˜„
```python
import torch
import torch.nn as nn
import torch.optim as optim

# PyTorchë¡œ ê°„ë‹¨í•œ ì‹ ê²½ë§ ì •ì˜
class SimpleNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.softmax = nn.Softmax(dim=1)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.softmax(x)
        return x

# ëª¨ë¸ ìƒì„±
model = SimpleNN(input_size=10, hidden_size=20, output_size=3)

# ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# í•™ìŠµ ê³¼ì •
def train_step(model, data, target):
    # ìˆœì „íŒŒ
    output = model(data)
    loss = criterion(output, target)
    
    # ì—­ì „íŒŒ
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    return loss.item()
```

## ğŸ“ ìì—°ì–´ ì²˜ë¦¬ (NLP)

### NLP ì£¼ìš” ì‘ì—…
```mermaid
graph TB
    subgraph "NLP Tasks"
        TOK[í† í°í™”<br/>Tokenization]
        EMB[ì„ë² ë”©<br/>Embedding]
        POS[í’ˆì‚¬ íƒœê¹…<br/>POS Tagging]
        NER[ê°œì²´ëª… ì¸ì‹<br/>NER]
        SENT[ê°ì • ë¶„ì„<br/>Sentiment]
        TRANS[ë²ˆì—­<br/>Translation]
        SUM[ìš”ì•½<br/>Summarization]
        QA[ì§ˆì˜ì‘ë‹µ<br/>Q&A]
    end
```

### í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
```python
import re
from konlpy.tag import Okt  # í•œêµ­ì–´ ì²˜ë¦¬

class TextPreprocessor:
    def __init__(self):
        self.okt = Okt()
    
    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\s]', '', text)
        
        # ê³µë°± ì •ê·œí™”
        text = ' '.join(text.split())
        
        return text.lower()
    
    def tokenize_korean(self, text):
        """í•œêµ­ì–´ í† í°í™”"""
        return self.okt.morphs(text)
    
    def tokenize_english(self, text):
        """ì˜ì–´ í† í°í™”"""
        return text.lower().split()

# ì‚¬ìš© ì˜ˆ
preprocessor = TextPreprocessor()
text = "ì•ˆë…•í•˜ì„¸ìš”! NLPë¥¼ ê³µë¶€í•˜ê³  ìˆìŠµë‹ˆë‹¤."
tokens = preprocessor.tokenize_korean(text)
print(tokens)  # ['ì•ˆë…•', 'í•˜', 'ì„¸ìš”', '!', 'NLP', 'ë¥¼', 'ê³µë¶€', 'í•˜ê³ ', 'ìˆìŠµë‹ˆë‹¤', '.']
```

### ì›Œë“œ ì„ë² ë”©
```python
# Word2Vec ì˜ˆì‹œ
from gensim.models import Word2Vec

# ë¬¸ì¥ ë°ì´í„°
sentences = [
    ['ë‚˜ëŠ”', 'íŒŒì´ì¬ì„', 'ì¢‹ì•„í•œë‹¤'],
    ['ë‚˜ëŠ”', 'ë¨¸ì‹ ëŸ¬ë‹ì„', 'ê³µë¶€í•œë‹¤'],
    ['íŒŒì´ì¬ì€', 'í”„ë¡œê·¸ë˜ë°', 'ì–¸ì–´ë‹¤'],
    ['ë¨¸ì‹ ëŸ¬ë‹ì€', 'ì¸ê³µì§€ëŠ¥ì˜', 'í•œ', 'ë¶„ì•¼ë‹¤']
]

# Word2Vec ëª¨ë¸ í•™ìŠµ
model = Word2Vec(
    sentences,
    vector_size=100,  # ì„ë² ë”© ì°¨ì›
    window=5,          # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°
    min_count=1,       # ìµœì†Œ ë“±ì¥ íšŸìˆ˜
    workers=4          # ë³‘ë ¬ ì²˜ë¦¬
)

# ë‹¨ì–´ ë²¡í„° ì–»ê¸°
vector = model.wv['íŒŒì´ì¬']
print(f"'íŒŒì´ì¬' ë²¡í„° ì°¨ì›: {vector.shape}")

# ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸°
similar_words = model.wv.most_similar('íŒŒì´ì¬', topn=3)
print(f"'íŒŒì´ì¬'ê³¼ ìœ ì‚¬í•œ ë‹¨ì–´: {similar_words}")
```

## ğŸš€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ (LLM)

### LLM ì•„í‚¤í…ì²˜
```mermaid
graph TB
    subgraph "Transformer Architecture"
        INPUT[ì…ë ¥ í…ìŠ¤íŠ¸]
        TOK[í† í°í™”]
        EMB[ì„ë² ë”©]
        PE[ìœ„ì¹˜ ì¸ì½”ë”©]
        
        subgraph "Transformer Blocks"
            ATT[Multi-Head Attention]
            FFN[Feed Forward Network]
            NORM[Layer Normalization]
        end
        
        OUTPUT[ì¶œë ¥ í…ìŠ¤íŠ¸]
        
        INPUT --> TOK
        TOK --> EMB
        EMB --> PE
        PE --> ATT
        ATT --> FFN
        FFN --> NORM
        NORM --> OUTPUT
    end
```

### í”„ë¡œì íŠ¸ì˜ LLM êµ¬í˜„
```python
# backend/llm/llm_service.py
from llama_cpp import Llama
import json

class LLMService:
    def __init__(self, model_path="models/mistral-7b-instruct-v0.2.Q4_K_M.gguf"):
        """Mistral 7B ëª¨ë¸ ì´ˆê¸°í™”"""
        self.model = Llama(
            model_path=model_path,
            n_ctx=4096,        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
            n_threads=4,       # CPU ìŠ¤ë ˆë“œ
            n_gpu_layers=32,   # GPU ë ˆì´ì–´ (ê°€ì†)
            verbose=False
        )
        
        # ìƒì„± íŒŒë¼ë¯¸í„°
        self.default_params = {
            'max_tokens': 512,
            'temperature': 0.7,
            'top_p': 0.95,
            'top_k': 40,
            'repeat_penalty': 1.1
        }
    
    def generate(self, prompt, **kwargs):
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        # íŒŒë¼ë¯¸í„° ë³‘í•©
        params = {**self.default_params, **kwargs}
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©
        formatted_prompt = self._apply_template(prompt)
        
        # ìƒì„±
        response = self.model(formatted_prompt, **params)
        
        return response['choices'][0]['text']
    
    def _apply_template(self, prompt):
        """Mistral Instruct í…œí”Œë¦¿"""
        return f"<s>[INST] {prompt} [/INST]"
    
    def generate_stream(self, prompt, **kwargs):
        """ìŠ¤íŠ¸ë¦¬ë° ìƒì„±"""
        params = {**self.default_params, **kwargs, 'stream': True}
        formatted_prompt = self._apply_template(prompt)
        
        for output in self.model(formatted_prompt, **params):
            yield output['choices'][0]['text']
```

### í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
```python
class PromptEngineering:
    """íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸ ì‘ì„± ê¸°ë²•"""
    
    @staticmethod
    def zero_shot(task, input_text):
        """Zero-shot: ì˜ˆì‹œ ì—†ì´"""
        return f"{task}\n\nì…ë ¥: {input_text}\nì¶œë ¥:"
    
    @staticmethod
    def few_shot(task, examples, input_text):
        """Few-shot: ëª‡ ê°€ì§€ ì˜ˆì‹œ ì œê³µ"""
        prompt = f"{task}\n\n"
        
        for ex in examples:
            prompt += f"ì…ë ¥: {ex['input']}\n"
            prompt += f"ì¶œë ¥: {ex['output']}\n\n"
        
        prompt += f"ì…ë ¥: {input_text}\nì¶œë ¥:"
        return prompt
    
    @staticmethod
    def chain_of_thought(question):
        """Chain-of-Thought: ë‹¨ê³„ë³„ ì‚¬ê³ """
        return f"""ì§ˆë¬¸: {question}

ë‹¨ê³„ë³„ë¡œ ìƒê°í•´ë´…ì‹œë‹¤:
1. ë¨¼ì € ë¬¸ì œë¥¼ ì´í•´í•©ë‹ˆë‹¤.
2. í•„ìš”í•œ ì •ë³´ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
3. ë…¼ë¦¬ì ìœ¼ë¡œ ì¶”ë¡ í•©ë‹ˆë‹¤.
4. ê²°ë¡ ì„ ë„ì¶œí•©ë‹ˆë‹¤.

ë‹µë³€:"""
    
    @staticmethod
    def role_play(role, task):
        """ì—­í•  ë¶€ì—¬"""
        return f"""ë‹¹ì‹ ì€ {role}ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ íŠ¹ì§•:
- ì „ë¬¸ì ì´ê³  ì •í™•í•œ ë‹µë³€
- ì¹œì ˆí•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…
- ì‹¤ìš©ì ì¸ ì˜ˆì‹œ ì œê³µ

ì‘ì—…: {task}"""

# ì‚¬ìš© ì˜ˆì‹œ
prompt_eng = PromptEngineering()

# Zero-shot
prompt = prompt_eng.zero_shot(
    "ë‹¤ìŒ ë¬¸ì¥ì˜ ê°ì •ì„ ë¶„ì„í•˜ì„¸ìš”",
    "ì˜¤ëŠ˜ ì •ë§ ê¸°ë¶„ì´ ì¢‹ì•„ìš”!"
)

# Few-shot
examples = [
    {"input": "ë§›ìˆë‹¤", "output": "ê¸ì •"},
    {"input": "ë³„ë¡œì•¼", "output": "ë¶€ì •"}
]
prompt = prompt_eng.few_shot(
    "ê°ì • ë¶„ë¥˜",
    examples,
    "ì •ë§ í›Œë¥­í•´ìš”"
)
```

### RAG (Retrieval-Augmented Generation)
```python
# backend/llm/rag_service.py
import chromadb
from chromadb.utils import embedding_functions

class RAGService:
    """ê²€ìƒ‰ ì¦ê°• ìƒì„±"""
    
    def __init__(self):
        # ë²¡í„° DB ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(path="./chroma_db")
        
        # ì„ë² ë”© í•¨ìˆ˜
        self.embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="all-MiniLM-L6-v2"
        )
        
        # ì»¬ë ‰ì…˜ ìƒì„±
        self.collection = self.client.get_or_create_collection(
            name="knowledge_base",
            embedding_function=self.embedding_fn
        )
    
    def add_document(self, text, metadata=None):
        """ë¬¸ì„œ ì¶”ê°€"""
        doc_id = f"doc_{self.collection.count() + 1}"
        
        self.collection.add(
            documents=[text],
            metadatas=[metadata or {}],
            ids=[doc_id]
        )
        
        return doc_id
    
    def search(self, query, n_results=3):
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
        results = self.collection.query(
            query_texts=[query],
            n_results=n_results
        )
        
        return results['documents'][0]
    
    def generate_with_context(self, query, llm_service):
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ìƒì„±"""
        # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        relevant_docs = self.search(query)
        
        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n".join(relevant_docs)
        
        # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:

ì°¸ê³  ì •ë³´:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€:"""
        
        # 4. LLM ìƒì„±
        response = llm_service.generate(prompt)
        
        return response
```

### ëª¨ë¸ íŒŒì¸íŠœë‹
```python
# Fine-tuning ë°ì´í„° ì¤€ë¹„
class FineTuningDataset:
    def __init__(self):
        self.data = []
    
    def add_example(self, instruction, input_text, output):
        """í•™ìŠµ ë°ì´í„° ì¶”ê°€"""
        self.data.append({
            "instruction": instruction,
            "input": input_text,
            "output": output
        })
    
    def save_jsonl(self, filepath):
        """JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
        import json
        
        with open(filepath, 'w', encoding='utf-8') as f:
            for item in self.data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')

# ë°ì´í„°ì…‹ ìƒì„±
dataset = FineTuningDataset()

# ì˜ˆì‹œ ì¶”ê°€
dataset.add_example(
    instruction="ë‹¤ìŒ ì½”ë“œì˜ ë²„ê·¸ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
    input_text="def add(a, b): return a - b",
    output="ë²„ê·¸: í•¨ìˆ˜ëª…ì€ addì¸ë° ëº„ì…ˆì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. 'return a + b'ë¡œ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤."
)

dataset.add_example(
    instruction="íŒŒì´ì¬ ì½”ë“œë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    input_text="lambda x: x**2",
    output="ì…ë ¥ê°’ xë¥¼ ë°›ì•„ì„œ xì˜ ì œê³±ì„ ë°˜í™˜í•˜ëŠ” ëŒë‹¤ í•¨ìˆ˜ì…ë‹ˆë‹¤."
)

# ì €ì¥
dataset.save_jsonl("training_data.jsonl")
```

### ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
```python
from typing import List, Dict
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class LLMEvaluator:
    """LLM í‰ê°€ ë©”íŠ¸ë¦­"""
    
    @staticmethod
    def perplexity(model, text):
        """Perplexity ê³„ì‚°"""
        # í† í°í™”
        tokens = model.tokenize(text)
        
        # ë¡œê·¸ í™•ë¥  ê³„ì‚°
        log_probs = []
        for i in range(1, len(tokens)):
            context = tokens[:i]
            target = tokens[i]
            
            # ë‹¤ìŒ í† í° í™•ë¥ 
            probs = model.get_token_probabilities(context)
            log_prob = np.log(probs[target])
            log_probs.append(log_prob)
        
        # Perplexity
        avg_log_prob = np.mean(log_probs)
        perplexity = np.exp(-avg_log_prob)
        
        return perplexity
    
    @staticmethod
    def bleu_score(reference: str, hypothesis: str) -> float:
        """BLEU ìŠ¤ì½”ì–´ ê³„ì‚°"""
        from nltk.translate.bleu_score import sentence_bleu
        
        reference_tokens = reference.split()
        hypothesis_tokens = hypothesis.split()
        
        score = sentence_bleu(
            [reference_tokens],
            hypothesis_tokens
        )
        
        return score
    
    @staticmethod
    def semantic_similarity(text1: str, text2: str, embedding_model) -> float:
        """ì˜ë¯¸ì  ìœ ì‚¬ë„"""
        # ì„ë² ë”© ìƒì„±
        emb1 = embedding_model.encode([text1])
        emb2 = embedding_model.encode([text2])
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        similarity = cosine_similarity(emb1, emb2)[0][0]
        
        return similarity
```

## ğŸ”¬ ëª¨ë¸ ìµœì í™”

### Quantization (ì–‘ìí™”)
```python
"""
ëª¨ë¸ í¬ê¸° ì¤„ì´ê¸°
- FP32 (32bit) â†’ FP16 (16bit) â†’ INT8 (8bit) â†’ INT4 (4bit)
- í¬ê¸° ê°ì†Œ, ì†ë„ í–¥ìƒ, ì•½ê°„ì˜ ì •í™•ë„ ì†ì‹¤
"""

# Mistral 7B ëª¨ë¸ í¬ê¸° ë¹„êµ
model_sizes = {
    "Original (FP32)": "28 GB",
    "FP16": "14 GB",
    "Q8_0": "7.7 GB",
    "Q5_K_M": "5.1 GB",
    "Q4_K_M": "4.4 GB",  # í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©
    "Q3_K_M": "3.5 GB"
}

print("Mistral 7B ì–‘ìí™”ë³„ í¬ê¸°:")
for quant, size in model_sizes.items():
    print(f"  {quant}: {size}")
```

### ì¶”ë¡  ìµœì í™”
```python
class InferenceOptimizer:
    """ì¶”ë¡  ì†ë„ ìµœì í™”"""
    
    @staticmethod
    def batch_inference(model, prompts: List[str], batch_size=4):
        """ë°°ì¹˜ ì²˜ë¦¬"""
        results = []
        
        for i in range(0, len(prompts), batch_size):
            batch = prompts[i:i+batch_size]
            batch_results = model.generate_batch(batch)
            results.extend(batch_results)
        
        return results
    
    @staticmethod
    def caching_layer(model, cache_size=100):
        """ìºì‹±"""
        from functools import lru_cache
        
        @lru_cache(maxsize=cache_size)
        def cached_generate(prompt):
            return model.generate(prompt)
        
        return cached_generate
    
    @staticmethod
    def gpu_acceleration(model_path):
        """GPU ê°€ì†"""
        import torch
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Using device: {device}")
        
        if device == "cuda":
            # GPU ë ˆì´ì–´ ì„¤ì •
            n_gpu_layers = 32
        else:
            n_gpu_layers = 0
        
        return n_gpu_layers
```

## ğŸ“š ì°¸ê³  ìë£Œ

### AI/ML ê¸°ì´ˆ
- [Andrew Ngì˜ Machine Learning ì½”ìŠ¤](https://www.coursera.org/learn/machine-learning)
- [Deep Learning Specialization](https://www.deeplearning.ai/)
- [Fast.ai ì‹¤ìš© ë”¥ëŸ¬ë‹](https://www.fast.ai/)

### NLP & LLM
- [Hugging Face Course](https://huggingface.co/learn/nlp-course)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [LangChain ë¬¸ì„œ](https://python.langchain.com/)

### í•œêµ­ì–´ NLP
- [KoNLPy ë¬¸ì„œ](https://konlpy.org/)
- [KoGPT í”„ë¡œì íŠ¸](https://github.com/kakaobrain/kogpt)
- [í•œêµ­ì–´ ì„ë² ë”©](https://ratsgo.github.io/embedding/)

### LLM ëª¨ë¸
- [Mistral AI](https://mistral.ai/)
- [LLaMA ëª¨ë¸](https://ai.meta.com/llama/)
- [OpenAI GPT](https://platform.openai.com/docs)

### ì‹¤ìŠµ ìë£Œ
- [Google Colab](https://colab.research.google.com/)
- [Kaggle Learn](https://www.kaggle.com/learn)
- [Papers with Code](https://paperswithcode.com/)

## ğŸ¯ í•µì‹¬ ì •ë¦¬

1. **AI/ML**ì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤
2. **ë”¥ëŸ¬ë‹**ì€ ì‹ ê²½ë§ì„ í†µí•´ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤
3. **LLM**ì€ ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ë¡œ í•™ìŠµëœ ì–¸ì–´ ì´í•´ ëª¨ë¸ì…ë‹ˆë‹¤
4. **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**ìœ¼ë¡œ LLM ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤
5. **RAG**ëŠ” ì™¸ë¶€ ì§€ì‹ì„ í™œìš©í•´ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤

---

ë‹¤ìŒ: [07-ê°œë°œë„êµ¬ì™€-í™˜ê²½ì„¤ì •.md](./07-ê°œë°œë„êµ¬ì™€-í™˜ê²½ì„¤ì •.md)