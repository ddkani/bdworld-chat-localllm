# 🚀 LLM 채팅 시스템 프로젝트 개요

## 📋 목차
1. [프로젝트 소개](#프로젝트-소개)
2. [전체 시스템 아키텍처](#전체-시스템-아키텍처)
3. [LLM 통합 구조](#llm-통합-구조)
4. [백엔드 구조](#백엔드-구조)
5. [프론트엔드 구조](#프론트엔드-구조)
6. [통신 플로우](#통신-플로우)
7. [핵심 기능](#핵심-기능)

---

## 🎯 프로젝트 소개

### 프로젝트 개요
**bdworld-chat-codefair**는 로컬에서 실행되는 LLM(대규모 언어 모델)을 활용한 실시간 채팅 시스템입니다.

### 주요 특징
- 🤖 **로컬 LLM 실행**: Mistral 7B 모델을 로컬에서 직접 실행
- 💬 **실시간 채팅**: WebSocket을 통한 실시간 양방향 통신
- 🔒 **프라이버시**: 모든 데이터가 로컬에서 처리
- 📱 **반응형 UI**: React 기반의 모던한 사용자 인터페이스
- 🎨 **관리자 대시보드**: 별도의 관리 인터페이스 제공

### 기술 스택
```mermaid
graph TB
    subgraph "Frontend"
        REACT[React 18]
        TS[TypeScript]
        MUI[Material-UI]
        WS_CLIENT[WebSocket Client]
    end
    
    subgraph "Backend"
        DJANGO[Django 5.0]
        CHANNELS[Django Channels]
        DRF[Django REST Framework]
    end
    
    subgraph "AI/ML"
        MISTRAL[Mistral 7B]
        LLAMA_CPP[llama-cpp-python]
        CHROMA[ChromaDB]
    end
    
    subgraph "Database"
        SQLITE[SQLite]
        REDIS[Redis]
    end
```

## 🏗️ 전체 시스템 아키텍처

### 시스템 구성도
```mermaid
graph TB
    subgraph "사용자 인터페이스"
        USER[사용자]
        BROWSER[웹 브라우저]
        USER --> BROWSER
    end
    
    subgraph "프론트엔드 애플리케이션"
        CHAT_UI[채팅 UI<br/>:3001]
        ADMIN_UI[관리자 UI<br/>:3002]
        BROWSER --> CHAT_UI
        BROWSER --> ADMIN_UI
    end
    
    subgraph "API Gateway"
        HTTP[HTTP 요청]
        WS[WebSocket 연결]
        CHAT_UI --> HTTP
        CHAT_UI --> WS
        ADMIN_UI --> HTTP
    end
    
    subgraph "백엔드 서버 [:8000]"
        DJANGO_APP[Django Application]
        ASGI[ASGI Server<br/>Daphne]
        WSGI[WSGI Server]
        
        HTTP --> WSGI
        WS --> ASGI
        WSGI --> DJANGO_APP
        ASGI --> DJANGO_APP
    end
    
    subgraph "LLM 처리"
        LLM_SERVICE[LLM Service]
        MODEL[Mistral 7B<br/>GGUF Format]
        DJANGO_APP --> LLM_SERVICE
        LLM_SERVICE --> MODEL
    end
    
    subgraph "데이터 저장소"
        DB[(SQLite)]
        VECTOR[(ChromaDB)]
        CACHE[(Redis)]
        DJANGO_APP --> DB
        LLM_SERVICE --> VECTOR
        ASGI --> CACHE
    end
    
    style USER fill:#e1f5fe
    style CHAT_UI fill:#c8e6c9
    style ADMIN_UI fill:#c8e6c9
    style DJANGO_APP fill:#fff3e0
    style LLM_SERVICE fill:#f3e5f5
    style MODEL fill:#f3e5f5
```

### 레이어별 책임

| 레이어 | 구성 요소 | 책임 |
|--------|-----------|------|
| **Presentation** | React UI | 사용자 인터페이스, 입력 처리, 결과 표시 |
| **API Gateway** | HTTP/WebSocket | 요청 라우팅, 프로토콜 변환 |
| **Application** | Django | 비즈니스 로직, 인증, 세션 관리 |
| **Service** | LLM Service | AI 모델 실행, 텍스트 생성 |
| **Data** | DB/Cache | 데이터 영속성, 캐싱 |

## 🤖 LLM 통합 구조

### LLM 실행 플로우
```mermaid
sequenceDiagram
    participant U as 사용자
    participant F as Frontend
    participant W as WebSocket
    participant B as Backend
    participant L as LLM Service
    participant M as Mistral Model
    participant D as Database
    
    U->>F: 메시지 입력
    F->>W: WebSocket 전송
    W->>B: 메시지 수신
    B->>D: 메시지 저장
    B->>L: 텍스트 생성 요청
    
    L->>L: 프롬프트 구성
    Note over L: 시스템 프롬프트 +<br/>대화 컨텍스트 +<br/>사용자 메시지
    
    L->>M: 모델 실행
    
    loop 스트리밍 생성
        M->>L: 토큰 생성
        L->>B: 청크 전달
        B->>W: 스트림 전송
        W->>F: 실시간 업데이트
        F->>U: 화면 표시
    end
    
    L->>B: 생성 완료
    B->>D: AI 응답 저장
    B->>W: 완료 신호
    W->>F: 스트림 종료
```

### LLM 서비스 구조
```python
# backend/llm/llm_service.py 구조

class LLMService:
    """LLM 통합 서비스"""
    
    def __init__(self):
        # 1. 모델 로드
        self.model = self._load_model()
        
        # 2. 설정 초기화
        self.config = {
            'max_tokens': 512,
            'temperature': 0.7,
            'context_length': 4096
        }
        
        # 3. 프롬프트 템플릿
        self.templates = PromptTemplates()
    
    def _load_model(self):
        """Mistral 7B 모델 로드"""
        return Llama(
            model_path="models/mistral-7b.gguf",
            n_ctx=4096,        # 컨텍스트 크기
            n_gpu_layers=32,   # GPU 레이어
            n_threads=8        # CPU 스레드
        )
    
    async def generate_stream(self, prompt, context):
        """스트리밍 텍스트 생성"""
        # 1. 프롬프트 구성
        full_prompt = self.templates.build(prompt, context)
        
        # 2. 토큰 수 확인
        if self.count_tokens(full_prompt) > 3500:
            context = self.truncate_context(context)
            full_prompt = self.templates.build(prompt, context)
        
        # 3. 스트리밍 생성
        for token in self.model(full_prompt, stream=True):
            yield token['choices'][0]['text']
```

### 모델 최적화 전략
```mermaid
graph LR
    subgraph "원본 모델"
        ORIG[Mistral 7B<br/>28GB FP32]
    end
    
    subgraph "양자화"
        QUANT[GGUF Q4_K_M<br/>4.3GB]
    end
    
    subgraph "실행 최적화"
        CPU[CPU 실행<br/>llama.cpp]
        GPU[GPU 가속<br/>CUDA/Metal]
    end
    
    subgraph "메모리 관리"
        CACHE[KV Cache]
        BATCH[배치 처리]
    end
    
    ORIG -->|양자화| QUANT
    QUANT --> CPU
    QUANT --> GPU
    CPU --> CACHE
    GPU --> BATCH
```

## 🔧 백엔드 구조

### Django 애플리케이션 구조
```mermaid
graph TB
    subgraph "Django Project"
        SETTINGS[settings.py<br/>전역 설정]
        URLS[urls.py<br/>URL 라우팅]
        ASGI_PY[asgi.py<br/>비동기 서버]
        WSGI_PY[wsgi.py<br/>동기 서버]
    end
    
    subgraph "Chat App"
        MODELS[models.py<br/>데이터 모델]
        VIEWS[views.py<br/>API 뷰]
        SERIAL[serializers.py<br/>직렬화]
        CONSUMERS[consumers.py<br/>WebSocket]
        ROUTING[routing.py<br/>WS 라우팅]
    end
    
    subgraph "LLM App"
        LLM_SVC[llm_service.py<br/>AI 서비스]
        LLM_MODELS[models.py<br/>AI 모델]
        LLM_VIEWS[views.py<br/>AI API]
    end
    
    URLS --> VIEWS
    ASGI_PY --> ROUTING
    ROUTING --> CONSUMERS
    VIEWS --> SERIAL
    SERIAL --> MODELS
    CONSUMERS --> LLM_SVC
```

### API 엔드포인트 구조
```python
# backend/chat_project/urls.py

urlpatterns = [
    # 인증 API
    path('api/auth/register/', RegisterView.as_view()),
    path('api/auth/login/', LoginView.as_view()),
    path('api/auth/logout/', LogoutView.as_view()),
    
    # 채팅 세션 API
    path('api/sessions/', ChatSessionViewSet.as_view({
        'get': 'list',      # 세션 목록
        'post': 'create'    # 새 세션
    })),
    path('api/sessions/<int:pk>/', ChatSessionViewSet.as_view({
        'get': 'retrieve',   # 세션 상세
        'put': 'update',     # 세션 수정
        'delete': 'destroy'  # 세션 삭제
    })),
    
    # 메시지 API
    path('api/sessions/<int:session_id>/messages/', MessageViewSet.as_view({
        'get': 'list',      # 메시지 목록
        'post': 'create'    # 메시지 전송
    })),
    
    # LLM API
    path('api/llm/generate/', LLMGenerateView.as_view()),
    path('api/llm/models/', LLMModelsView.as_view()),
]

# WebSocket 라우팅
websocket_urlpatterns = [
    path('ws/chat/<int:session_id>/', ChatConsumer.as_asgi()),
]
```

### 데이터 모델 관계
```mermaid
erDiagram
    User ||--o{ ChatSession : creates
    ChatSession ||--o{ Message : contains
    ChatSession ||--|| SessionConfig : has
    Message ||--|| MessageMetadata : has
    
    User {
        int id PK
        string username
        string email
        datetime created_at
    }
    
    ChatSession {
        int id PK
        int user_id FK
        string title
        datetime created_at
        datetime updated_at
    }
    
    Message {
        int id PK
        int session_id FK
        text content
        boolean is_user
        datetime timestamp
        int token_count
        float processing_time
    }
    
    SessionConfig {
        int id PK
        int session_id FK
        float temperature
        int max_tokens
        string system_prompt
    }
    
    MessageMetadata {
        int id PK
        int message_id FK
        json prompt_template
        json model_params
        string model_version
    }
```

## 💻 프론트엔드 구조

### React 컴포넌트 계층
```mermaid
graph TB
    subgraph "App Root"
        APP[App.tsx]
    end
    
    subgraph "Providers"
        AUTH_PROVIDER[AuthProvider]
        THEME_PROVIDER[ThemeProvider]
        WS_PROVIDER[WebSocketProvider]
    end
    
    subgraph "Pages"
        LOGIN[LoginPage]
        CHAT[ChatPage]
        ADMIN[AdminPage]
    end
    
    subgraph "Containers"
        CHAT_CONTAINER[ChatContainer]
        SESSION_LIST[SessionList]
        USER_PROFILE[UserProfile]
    end
    
    subgraph "Components"
        MESSAGE_LIST[MessageList]
        MESSAGE_ITEM[MessageItem]
        INPUT_BOX[InputBox]
        SIDEBAR[Sidebar]
    end
    
    subgraph "UI Components"
        BUTTON[Button]
        CARD[Card]
        DIALOG[Dialog]
        LOADING[LoadingSpinner]
    end
    
    APP --> AUTH_PROVIDER
    AUTH_PROVIDER --> THEME_PROVIDER
    THEME_PROVIDER --> WS_PROVIDER
    WS_PROVIDER --> CHAT
    
    CHAT --> CHAT_CONTAINER
    CHAT --> SESSION_LIST
    
    CHAT_CONTAINER --> MESSAGE_LIST
    CHAT_CONTAINER --> INPUT_BOX
    
    MESSAGE_LIST --> MESSAGE_ITEM
    MESSAGE_ITEM --> CARD
    INPUT_BOX --> BUTTON
```

### 상태 관리 구조
```typescript
// frontend-chat/src/store/structure.ts

interface AppState {
    // 인증 상태
    auth: {
        user: User | null;
        token: string | null;
        isAuthenticated: boolean;
    };
    
    // 채팅 상태
    chat: {
        sessions: ChatSession[];
        currentSession: ChatSession | null;
        messages: Map<number, Message[]>;
        isLoading: boolean;
    };
    
    // WebSocket 상태
    websocket: {
        connection: WebSocket | null;
        status: 'connecting' | 'connected' | 'disconnected';
        reconnectAttempts: number;
    };
    
    // UI 상태
    ui: {
        theme: 'light' | 'dark';
        sidebarOpen: boolean;
        notifications: Notification[];
    };
}
```

### 서비스 레이어
```typescript
// frontend-chat/src/services/structure.ts

class APIService {
    // HTTP 통신
    async get<T>(url: string): Promise<T>
    async post<T>(url: string, data: any): Promise<T>
    async put<T>(url: string, data: any): Promise<T>
    async delete(url: string): Promise<void>
}

class WebSocketService {
    // WebSocket 통신
    connect(sessionId: number): void
    disconnect(): void
    send(message: any): void
    on(event: string, callback: Function): void
    off(event: string, callback: Function): void
}

class ChatService {
    // 채팅 비즈니스 로직
    async createSession(title: string): Promise<ChatSession>
    async loadSessions(): Promise<ChatSession[]>
    async sendMessage(content: string): Promise<void>
    subscribeToMessages(callback: Function): void
}
```

## 🔄 통신 플로우

### HTTP 통신 플로우
```mermaid
sequenceDiagram
    participant C as Client
    participant API as API Service
    participant S as Django Server
    participant DB as Database
    
    Note over C,DB: 세션 생성 플로우
    
    C->>API: createSession(title)
    API->>API: 토큰 추가
    API->>S: POST /api/sessions/
    S->>S: 인증 확인
    S->>DB: 세션 저장
    DB->>S: 세션 ID
    S->>API: 201 Created + 세션 데이터
    API->>C: ChatSession 객체
    
    Note over C,DB: 메시지 조회 플로우
    
    C->>API: loadMessages(sessionId)
    API->>S: GET /api/sessions/{id}/messages/
    S->>DB: 메시지 쿼리
    DB->>S: 메시지 목록
    S->>API: 200 OK + 메시지 배열
    API->>C: Message[] 배열
```

### WebSocket 통신 플로우
```mermaid
sequenceDiagram
    participant C as Client
    participant WS as WebSocket
    participant CH as Channels
    participant CS as Consumer
    participant LLM as LLM Service
    
    Note over C,LLM: WebSocket 연결 및 메시지 전송
    
    C->>WS: new WebSocket(url)
    WS->>CH: 연결 요청
    CH->>CS: connect()
    CS->>CS: 인증 확인
    CS->>CH: accept()
    CH->>WS: 연결 수립
    WS->>C: onopen
    
    C->>WS: send({type: 'chat_message', message: 'Hello'})
    WS->>CH: 메시지 라우팅
    CH->>CS: receive_json()
    
    CS->>CS: 메시지 저장
    CS->>LLM: generate_stream(message)
    
    loop 스트리밍
        LLM->>CS: yield chunk
        CS->>CH: send_json({type: 'ai_chunk', chunk})
        CH->>WS: 프레임 전송
        WS->>C: onmessage(chunk)
    end
    
    CS->>CH: send_json({type: 'ai_complete'})
    CH->>WS: 완료 신호
    WS->>C: onmessage(complete)
```

### 에러 처리 플로우
```mermaid
graph TB
    subgraph "Client Error Handling"
        C_ERR[에러 발생]
        C_CATCH[try-catch]
        C_RETRY[재시도 로직]
        C_NOTIFY[사용자 알림]
    end
    
    subgraph "Network Error"
        NET_TIMEOUT[타임아웃]
        NET_DISCONNECT[연결 끊김]
        NET_RECONNECT[자동 재연결]
    end
    
    subgraph "Server Error"
        S_400[400 Bad Request]
        S_401[401 Unauthorized]
        S_500[500 Server Error]
    end
    
    subgraph "LLM Error"
        LLM_OOM[메모리 부족]
        LLM_TIMEOUT[생성 타임아웃]
        LLM_FALLBACK[폴백 처리]
    end
    
    C_ERR --> C_CATCH
    C_CATCH --> C_RETRY
    C_RETRY --> C_NOTIFY
    
    NET_TIMEOUT --> NET_RECONNECT
    NET_DISCONNECT --> NET_RECONNECT
    
    S_401 --> C_NOTIFY
    S_500 --> C_RETRY
    
    LLM_OOM --> LLM_FALLBACK
    LLM_TIMEOUT --> LLM_FALLBACK
```

## 🎨 핵심 기능

### 1. 실시간 채팅
```mermaid
graph LR
    subgraph "기능"
        RT_MSG[실시간 메시지]
        TYPING[타이핑 표시]
        PRESENCE[온라인 상태]
        NOTIF[알림]
    end
    
    subgraph "기술"
        WS_CONN[WebSocket]
        CHANNELS[Django Channels]
        REDIS_PS[Redis Pub/Sub]
    end
    
    RT_MSG --> WS_CONN
    TYPING --> WS_CONN
    PRESENCE --> REDIS_PS
    NOTIF --> CHANNELS
```

### 2. AI 응답 생성
```mermaid
graph TB
    subgraph "입력 처리"
        USER_INPUT[사용자 입력]
        VALIDATE[검증]
        PREPROCESS[전처리]
    end
    
    subgraph "AI 처리"
        CONTEXT[컨텍스트 구성]
        PROMPT[프롬프트 생성]
        GENERATE[텍스트 생성]
        STREAM[스트리밍]
    end
    
    subgraph "출력 처리"
        FORMAT[포맷팅]
        MARKDOWN[마크다운 렌더링]
        CODE_HL[코드 하이라이팅]
    end
    
    USER_INPUT --> VALIDATE
    VALIDATE --> PREPROCESS
    PREPROCESS --> CONTEXT
    CONTEXT --> PROMPT
    PROMPT --> GENERATE
    GENERATE --> STREAM
    STREAM --> FORMAT
    FORMAT --> MARKDOWN
    FORMAT --> CODE_HL
```

### 3. 세션 관리
```typescript
// 세션 관리 기능
interface SessionManager {
    // 세션 생성/삭제
    createSession(title: string): Promise<Session>
    deleteSession(id: number): Promise<void>
    
    // 세션 전환
    switchSession(id: number): void
    getCurrentSession(): Session | null
    
    // 세션 내보내기/가져오기
    exportSession(id: number): Promise<ExportData>
    importSession(data: ExportData): Promise<Session>
    
    // 세션 검색
    searchSessions(query: string): Session[]
    getRecentSessions(limit: number): Session[]
}
```

### 4. 사용자 관리
```python
# 사용자 관리 기능
class UserManager:
    def register(self, username: str, email: str) -> User
    def login(self, username: str) -> AuthToken
    def logout(self, token: str) -> None
    
    # 프로필 관리
    def get_profile(self, user_id: int) -> UserProfile
    def update_profile(self, user_id: int, data: dict) -> UserProfile
    
    # 사용 통계
    def get_usage_stats(self, user_id: int) -> UsageStats
    def get_message_history(self, user_id: int) -> List[Message]
```

## 📊 성능 최적화

### 백엔드 최적화
```python
# 1. 데이터베이스 쿼리 최적화
sessions = ChatSession.objects.select_related('user') \
                              .prefetch_related('messages') \
                              .filter(user=request.user)

# 2. 캐싱 전략
from django.core.cache import cache

def get_session_with_cache(session_id):
    cache_key = f'session_{session_id}'
    session = cache.get(cache_key)
    
    if session is None:
        session = ChatSession.objects.get(id=session_id)
        cache.set(cache_key, session, 300)  # 5분 캐싱
    
    return session

# 3. 비동기 처리
async def process_message_async(message):
    # 비동기 DB 쿼리
    session = await ChatSession.objects.aget(id=message.session_id)
    
    # 비동기 LLM 처리
    async for chunk in llm_service.generate_async(message.content):
        await send_to_client(chunk)
```

### 프론트엔드 최적화
```typescript
// 1. React 메모이제이션
const MessageList = React.memo(({ messages }) => {
    return messages.map(msg => <MessageItem key={msg.id} {...msg} />)
}, (prev, next) => prev.messages.length === next.messages.length);

// 2. 가상 스크롤링
import { FixedSizeList } from 'react-window';

const VirtualMessageList = ({ messages }) => (
    <FixedSizeList
        height={600}
        itemCount={messages.length}
        itemSize={80}
        width="100%"
    >
        {({ index, style }) => (
            <div style={style}>
                <MessageItem {...messages[index]} />
            </div>
        )}
    </FixedSizeList>
);

// 3. 디바운싱
const debouncedSearch = useMemo(
    () => debounce((query: string) => {
        searchSessions(query);
    }, 300),
    []
);
```

### LLM 최적화
```python
# 1. 모델 캐싱
class ModelCache:
    _instance = None
    _model = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._model = Llama(model_path="model.gguf")
        return cls._instance

# 2. 배치 처리
def batch_generate(prompts: List[str]):
    results = []
    for batch in chunks(prompts, batch_size=4):
        batch_results = model.generate_batch(batch)
        results.extend(batch_results)
    return results

# 3. KV 캐시 관리
model = Llama(
    model_path="model.gguf",
    n_ctx=4096,
    use_mmap=True,      # 메모리 매핑
    use_mlock=True,     # 메모리 고정
    n_batch=512,        # 배치 크기
    n_threads=8         # 병렬 처리
)
```

## 🔒 보안 고려사항

### 인증 및 권한
```mermaid
graph TB
    subgraph "인증 플로우"
        LOGIN[로그인]
        TOKEN[토큰 발급]
        VERIFY[토큰 검증]
        ACCESS[접근 허용]
    end
    
    subgraph "보안 계층"
        CORS[CORS 설정]
        CSRF[CSRF 보호]
        XSS[XSS 방지]
        SQL_INJ[SQL Injection 방지]
    end
    
    subgraph "데이터 보호"
        ENCRYPT[암호화]
        HASH[해싱]
        SANITIZE[살균처리]
    end
    
    LOGIN --> TOKEN
    TOKEN --> VERIFY
    VERIFY --> ACCESS
    
    ACCESS --> CORS
    ACCESS --> CSRF
    ACCESS --> XSS
    ACCESS --> SQL_INJ
    
    ENCRYPT --> HASH
    HASH --> SANITIZE
```

## 📚 참고 자료

### 프로젝트 문서
- [웹 개발 기초](./01-웹개발-기초개념.md)
- [네트워크와 통신](./02-네트워크와-통신-기초.md)
- [프로젝트 구조 심화](./08-프로젝트-구조-심화.md)
- [LLM 로컬 실행](./10-LLM-로컬실행-상세.md)

### 외부 참고 자료
- [Django 공식 문서](https://docs.djangoproject.com/)
- [React 공식 문서](https://react.dev/)
- [WebSocket MDN](https://developer.mozilla.org/ko/docs/Web/API/WebSocket)
- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)

## 🎯 핵심 정리

1. **통합 아키텍처**: LLM + 백엔드 + 프론트엔드가 유기적으로 연결
2. **실시간 통신**: WebSocket을 통한 양방향 실시간 통신
3. **로컬 AI**: 프라이버시를 보장하는 로컬 LLM 실행
4. **확장 가능**: 모듈화된 구조로 기능 추가 용이
5. **최적화**: 각 레이어별 성능 최적화 적용

---

다음: [01-웹개발-기초개념.md](./01-웹개발-기초개념.md)