# 🤖 AI 모델 통합 이해하기

## 📌 LLM (Large Language Model)이란?

**대규모 언어 모델**은 방대한 텍스트 데이터로 학습된 AI입니다.
- 인간처럼 자연스러운 대화 가능
- 질문에 답하고, 글을 쓰고, 번역 가능
- GPT, Claude, Mistral 등이 대표적

### 모델 크기와 성능
```
모델 파라미터 수:
- 7B (70억개) = 약 4-8GB 메모리
- 13B (130억개) = 약 8-16GB 메모리
- 70B (700억개) = 약 40-80GB 메모리

더 큰 모델 = 더 똑똑하지만 더 느림
```

## 🎯 Mistral 7B 모델

### 특징
- **오픈소스** 모델
- 70억 개 파라미터
- **Quantization** (양자화)로 크기 축소
- 로컬에서 실행 가능

### GGUF 형식
```
원본 모델: 14GB
     ↓ Quantization (압축)
GGUF 모델: 4GB (Q4_K_M)

품질은 약간 낮아지지만 속도는 빨라짐
```

## 🔧 백엔드 AI 통합

### 1. llama-cpp-python 설치
```python
# requirements.txt
llama-cpp-python==0.2.90

# Mac (Metal 가속)
CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python

# Windows (CPU)
pip install llama-cpp-python

# Linux (CUDA)
CMAKE_ARGS="-DLLAMA_CUDA=on" pip install llama-cpp-python
```

### 2. LLM 서비스 구현
```python
# llm/llm_service.py
from llama_cpp import Llama
import json
from typing import Generator, Optional, Dict, Any

class LLMService:
    def __init__(self):
        self.model = None
        self.context_length = 4096
        self.max_tokens = 512
        self.temperature = 0.7
        self.load_model()
    
    def load_model(self):
        """모델 로드"""
        try:
            self.model = Llama(
                model_path="models/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
                n_ctx=self.context_length,  # 컨텍스트 길이
                n_threads=4,  # CPU 스레드 수
                n_gpu_layers=32,  # GPU 레이어 (Mac Metal/CUDA)
                verbose=False
            )
            print("✅ 모델 로드 성공")
        except Exception as e:
            print(f"❌ 모델 로드 실패: {e}")
    
    def generate(self, prompt: str, **kwargs) -> str:
        """텍스트 생성 (일반)"""
        if not self.model:
            return "모델이 로드되지 않았습니다."
        
        # 프롬프트 템플릿 적용
        formatted_prompt = self.apply_template(prompt)
        
        # 생성 매개변수
        params = {
            'max_tokens': kwargs.get('max_tokens', self.max_tokens),
            'temperature': kwargs.get('temperature', self.temperature),
            'top_p': kwargs.get('top_p', 0.95),
            'top_k': kwargs.get('top_k', 40),
            'repeat_penalty': kwargs.get('repeat_penalty', 1.1),
            'stop': kwargs.get('stop', ["</s>", "\n\n"])
        }
        
        # 텍스트 생성
        response = self.model(
            formatted_prompt,
            **params
        )
        
        return response['choices'][0]['text'].strip()
    
    def generate_stream(self, prompt: str, **kwargs) -> Generator:
        """텍스트 생성 (스트리밍)"""
        if not self.model:
            yield "모델이 로드되지 않았습니다."
            return
        
        formatted_prompt = self.apply_template(prompt)
        
        params = {
            'max_tokens': kwargs.get('max_tokens', self.max_tokens),
            'temperature': kwargs.get('temperature', self.temperature),
            'top_p': kwargs.get('top_p', 0.95),
            'stream': True  # 스트리밍 활성화
        }
        
        # 스트리밍 생성
        stream = self.model(
            formatted_prompt,
            **params
        )
        
        for output in stream:
            chunk = output['choices'][0]['text']
            if chunk:
                yield chunk
    
    def apply_template(self, prompt: str) -> str:
        """프롬프트 템플릿 적용"""
        # Mistral Instruct 템플릿
        template = f"""<s>[INST] {prompt} [/INST]"""
        return template
```

### 3. 프롬프트 템플릿 관리
```python
# llm/models.py
from django.db import models

class PromptTemplate(models.Model):
    name = models.CharField(max_length=100)
    system_prompt = models.TextField(
        help_text="시스템 프롬프트 (AI의 역할 정의)"
    )
    user_template = models.TextField(
        help_text="사용자 메시지 템플릿"
    )
    assistant_template = models.TextField(
        help_text="AI 응답 템플릿"
    )
    is_active = models.BooleanField(default=False)
    created_at = models.DateTimeField(auto_now_add=True)
    
    # Few-shot 예제
    examples = models.JSONField(
        default=list,
        help_text="대화 예제들"
    )
    
    def format_prompt(self, user_message: str) -> str:
        """프롬프트 포맷팅"""
        prompt = self.system_prompt + "\n\n"
        
        # Few-shot 예제 추가
        for example in self.examples:
            prompt += f"User: {example['user']}\n"
            prompt += f"Assistant: {example['assistant']}\n\n"
        
        # 실제 사용자 메시지
        prompt += f"User: {user_message}\nAssistant:"
        
        return prompt
```

### 4. RAG (Retrieval-Augmented Generation)
```python
# llm/rag_service.py
import chromadb
from chromadb.utils import embedding_functions
from typing import List, Dict

class RAGService:
    def __init__(self):
        # ChromaDB 클라이언트 초기화
        self.client = chromadb.PersistentClient(
            path="./rag_vectors.db"
        )
        
        # 임베딩 함수 (문장을 벡터로 변환)
        self.embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="all-MiniLM-L6-v2"
        )
        
        # 컬렉션 생성/로드
        self.collection = self.client.get_or_create_collection(
            name="documents",
            embedding_function=self.embedding_fn
        )
    
    def add_document(self, text: str, metadata: Dict = None):
        """문서 추가"""
        doc_id = f"doc_{self.collection.count() + 1}"
        
        self.collection.add(
            documents=[text],
            metadatas=[metadata or {}],
            ids=[doc_id]
        )
        
        return doc_id
    
    def search(self, query: str, n_results: int = 3) -> List[Dict]:
        """유사 문서 검색"""
        results = self.collection.query(
            query_texts=[query],
            n_results=n_results
        )
        
        documents = []
        for i in range(len(results['documents'][0])):
            documents.append({
                'text': results['documents'][0][i],
                'metadata': results['metadatas'][0][i],
                'distance': results['distances'][0][i]
            })
        
        return documents
    
    def generate_with_context(self, 
                             query: str, 
                             llm_service) -> str:
        """컨텍스트를 포함한 응답 생성"""
        # 관련 문서 검색
        relevant_docs = self.search(query, n_results=3)
        
        # 컨텍스트 구성
        context = "\n\n".join([
            f"참고 문서 {i+1}: {doc['text']}"
            for i, doc in enumerate(relevant_docs)
        ])
        
        # 프롬프트 구성
        prompt = f"""다음 참고 문서를 바탕으로 질문에 답해주세요.

{context}

질문: {query}

답변:"""
        
        # LLM으로 응답 생성
        response = llm_service.generate(prompt)
        
        return response, relevant_docs
```

## 🎛️ AI 설정 관리

### 1. 모델 파라미터
```python
# 생성 파라미터 설명
class GenerationParams:
    temperature: float = 0.7  # 창의성 (0=보수적, 1=창의적)
    max_tokens: int = 512     # 최대 토큰 수
    top_p: float = 0.95      # 누적 확률 임계값
    top_k: int = 40          # 상위 K개 토큰만 고려
    repeat_penalty: float = 1.1  # 반복 페널티
    
    def to_dict(self):
        return {
            'temperature': self.temperature,
            'max_tokens': self.max_tokens,
            'top_p': self.top_p,
            'top_k': self.top_k,
            'repeat_penalty': self.repeat_penalty
        }
```

### 2. 프롬프트 엔지니어링
```python
# 다양한 프롬프트 템플릿
PROMPT_TEMPLATES = {
    'default': {
        'system': "당신은 도움이 되는 AI 어시스턴트입니다.",
        'format': "{system}\n\nUser: {user}\nAssistant:"
    },
    
    'coding': {
        'system': "당신은 전문 프로그래머입니다. 코드를 명확하고 효율적으로 작성합니다.",
        'format': "{system}\n\n요구사항: {user}\n\n코드:"
    },
    
    'teacher': {
        'system': "당신은 친절한 선생님입니다. 복잡한 개념을 쉽게 설명합니다.",
        'format': "{system}\n\n학생 질문: {user}\n\n선생님 답변:"
    },
    
    'creative': {
        'system': "당신은 창의적인 작가입니다.",
        'format': "{system}\n\n주제: {user}\n\n창작물:"
    }
}
```

## 💾 Fine-tuning (미세 조정)

### 1. 데이터셋 준비
```python
# 학습 데이터 형식 (JSONL)
{
  "instruction": "파이썬에서 리스트를 정렬하는 방법을 알려주세요",
  "input": "",
  "output": "파이썬에서 리스트를 정렬하는 방법:\n1. sort() 메서드: list.sort()\n2. sorted() 함수: sorted_list = sorted(list)"
}
```

### 2. Fine-tuning 프로세스
```python
# llm/finetuning_service.py
import json
import subprocess
from pathlib import Path

class FineTuningService:
    def __init__(self):
        self.base_model_path = "models/mistral-7b.gguf"
        self.output_dir = "models/finetuned"
    
    def prepare_dataset(self, raw_data: List[Dict]) -> str:
        """데이터셋 준비"""
        dataset_path = "datasets/training_data.jsonl"
        
        with open(dataset_path, 'w', encoding='utf-8') as f:
            for item in raw_data:
                # Alpaca 형식으로 변환
                formatted = {
                    "instruction": item.get("instruction", ""),
                    "input": item.get("input", ""),
                    "output": item.get("output", "")
                }
                f.write(json.dumps(formatted, ensure_ascii=False) + '\n')
        
        return dataset_path
    
    def start_training(self, dataset_path: str, **kwargs):
        """학습 시작"""
        config = {
            'learning_rate': kwargs.get('learning_rate', 3e-4),
            'batch_size': kwargs.get('batch_size', 4),
            'num_epochs': kwargs.get('num_epochs', 3),
            'warmup_steps': kwargs.get('warmup_steps', 100),
        }
        
        # LoRA 설정 (Low-Rank Adaptation)
        lora_config = {
            'r': 8,  # LoRA rank
            'lora_alpha': 16,
            'lora_dropout': 0.05,
            'target_modules': ["q_proj", "v_proj"]
        }
        
        # 학습 명령어 실행 (예시)
        cmd = [
            "python", "train.py",
            "--model_path", self.base_model_path,
            "--data_path", dataset_path,
            "--output_dir", self.output_dir,
            "--learning_rate", str(config['learning_rate']),
            "--batch_size", str(config['batch_size']),
            "--num_epochs", str(config['num_epochs'])
        ]
        
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        
        return process
```

## 🔍 모델 성능 최적화

### 1. 캐싱
```python
from functools import lru_cache
import hashlib

class CachedLLMService(LLMService):
    @lru_cache(maxsize=100)
    def generate_cached(self, prompt_hash: str, **kwargs):
        """캐시된 생성"""
        return super().generate(prompt_hash, **kwargs)
    
    def generate(self, prompt: str, **kwargs):
        # 프롬프트 해시 생성
        prompt_hash = hashlib.md5(
            prompt.encode()
        ).hexdigest()
        
        # 캐시 확인
        if kwargs.get('use_cache', True):
            return self.generate_cached(prompt_hash, **kwargs)
        
        return super().generate(prompt, **kwargs)
```

### 2. 배치 처리
```python
async def batch_generate(self, prompts: List[str]) -> List[str]:
    """여러 프롬프트 동시 처리"""
    import asyncio
    
    tasks = [
        self.generate_async(prompt) 
        for prompt in prompts
    ]
    
    responses = await asyncio.gather(*tasks)
    return responses
```

### 3. GPU 가속
```python
# GPU 사용 설정
def setup_gpu():
    import torch
    
    if torch.cuda.is_available():
        device = "cuda"
        gpu_layers = 32
    elif torch.backends.mps.is_available():  # Mac M1/M2
        device = "mps"
        gpu_layers = 1
    else:
        device = "cpu"
        gpu_layers = 0
    
    return device, gpu_layers
```

## 📊 모델 모니터링

### 1. 성능 메트릭
```python
class ModelMetrics:
    def __init__(self):
        self.total_requests = 0
        self.total_tokens = 0
        self.response_times = []
    
    def log_request(self, 
                   prompt_tokens: int,
                   completion_tokens: int,
                   response_time: float):
        """요청 로깅"""
        self.total_requests += 1
        self.total_tokens += prompt_tokens + completion_tokens
        self.response_times.append(response_time)
    
    def get_stats(self):
        """통계 반환"""
        return {
            'total_requests': self.total_requests,
            'total_tokens': self.total_tokens,
            'avg_response_time': sum(self.response_times) / len(self.response_times),
            'tokens_per_second': self.total_tokens / sum(self.response_times)
        }
```

### 2. 품질 평가
```python
def evaluate_response(response: str, expected: str = None):
    """응답 품질 평가"""
    metrics = {
        'length': len(response),
        'has_code': '```' in response,
        'politeness': any(word in response.lower() 
                         for word in ['감사', '죄송', '안녕']),
        'relevance': 0.0  # 관련성 점수
    }
    
    if expected:
        # 유사도 계산
        from difflib import SequenceMatcher
        metrics['relevance'] = SequenceMatcher(
            None, response, expected
        ).ratio()
    
    return metrics
```

## 📚 학습 자료

### LLM 이해
- [LLM 입문 가이드](https://www.promptingguide.ai/kr)
- [Hugging Face 강좌](https://huggingface.co/learn/nlp-course/chapter1/1)
- [LangChain 문서](https://python.langchain.com/docs/get_started/introduction)

### 모델 활용
- [llama.cpp 문서](https://github.com/ggerganov/llama.cpp)
- [Mistral AI 문서](https://docs.mistral.ai/)
- [OpenAI Cookbook](https://cookbook.openai.com/)

### 프롬프트 엔지니어링
- [프롬프트 엔지니어링 가이드](https://www.promptingguide.ai/kr/introduction/basics)
- [ChatGPT Prompt Engineering](https://platform.openai.com/docs/guides/prompt-engineering)

### Fine-tuning
- [LoRA 논문 이해하기](https://arxiv.org/abs/2106.09685)
- [QLoRA 튜토리얼](https://github.com/artidoro/qlora)

## 💡 실습 아이디어

1. **프롬프트 템플릿 만들기**
   - 다양한 역할의 AI 만들기
   - Few-shot 학습 실험
   - Chain-of-Thought 구현

2. **RAG 시스템 구축**
   - PDF 문서 임베딩
   - 검색 정확도 개선
   - 하이브리드 검색

3. **모델 최적화**
   - 응답 속도 개선
   - 메모리 사용량 줄이기
   - 배치 처리 구현

## 🤔 생각해볼 문제

1. AI의 환각(Hallucination)을 어떻게 줄일까?
2. 모델의 편향성을 어떻게 해결할까?
3. 개인정보 보호는 어떻게 할까?

---

다음 문서: [07-시스템-아키텍처.md](./07-시스템-아키텍처.md) 에서 전체 시스템 구조를 시각화해봐요!