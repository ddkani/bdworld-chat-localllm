# ğŸ¤– AI ëª¨ë¸ í†µí•© ì´í•´í•˜ê¸°

## ğŸ“Œ LLM (Large Language Model)ì´ë€?

**ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸**ì€ ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ í•™ìŠµëœ AIì…ë‹ˆë‹¤.
- ì¸ê°„ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ê°€ëŠ¥
- ì§ˆë¬¸ì— ë‹µí•˜ê³ , ê¸€ì„ ì“°ê³ , ë²ˆì—­ ê°€ëŠ¥
- GPT, Claude, Mistral ë“±ì´ ëŒ€í‘œì 

### ëª¨ë¸ í¬ê¸°ì™€ ì„±ëŠ¥
```
ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜:
- 7B (70ì–µê°œ) = ì•½ 4-8GB ë©”ëª¨ë¦¬
- 13B (130ì–µê°œ) = ì•½ 8-16GB ë©”ëª¨ë¦¬
- 70B (700ì–µê°œ) = ì•½ 40-80GB ë©”ëª¨ë¦¬

ë” í° ëª¨ë¸ = ë” ë˜‘ë˜‘í•˜ì§€ë§Œ ë” ëŠë¦¼
```

## ğŸ¯ Mistral 7B ëª¨ë¸

### íŠ¹ì§•
- **ì˜¤í”ˆì†ŒìŠ¤** ëª¨ë¸
- 70ì–µ ê°œ íŒŒë¼ë¯¸í„°
- **Quantization** (ì–‘ìí™”)ë¡œ í¬ê¸° ì¶•ì†Œ
- ë¡œì»¬ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥

### GGUF í˜•ì‹
```
ì›ë³¸ ëª¨ë¸: 14GB
     â†“ Quantization (ì••ì¶•)
GGUF ëª¨ë¸: 4GB (Q4_K_M)

í’ˆì§ˆì€ ì•½ê°„ ë‚®ì•„ì§€ì§€ë§Œ ì†ë„ëŠ” ë¹¨ë¼ì§
```

## ğŸ”§ ë°±ì—”ë“œ AI í†µí•©

### 1. llama-cpp-python ì„¤ì¹˜
```python
# requirements.txt
llama-cpp-python==0.2.90

# Mac (Metal ê°€ì†)
CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python

# Windows (CPU)
pip install llama-cpp-python

# Linux (CUDA)
CMAKE_ARGS="-DLLAMA_CUDA=on" pip install llama-cpp-python
```

### 2. LLM ì„œë¹„ìŠ¤ êµ¬í˜„
```python
# llm/llm_service.py
from llama_cpp import Llama
import json
from typing import Generator, Optional, Dict, Any

class LLMService:
    def __init__(self):
        self.model = None
        self.context_length = 4096
        self.max_tokens = 512
        self.temperature = 0.7
        self.load_model()
    
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            self.model = Llama(
                model_path="models/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
                n_ctx=self.context_length,  # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
                n_threads=4,  # CPU ìŠ¤ë ˆë“œ ìˆ˜
                n_gpu_layers=32,  # GPU ë ˆì´ì–´ (Mac Metal/CUDA)
                verbose=False
            )
            print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def generate(self, prompt: str, **kwargs) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„± (ì¼ë°˜)"""
        if not self.model:
            return "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©
        formatted_prompt = self.apply_template(prompt)
        
        # ìƒì„± ë§¤ê°œë³€ìˆ˜
        params = {
            'max_tokens': kwargs.get('max_tokens', self.max_tokens),
            'temperature': kwargs.get('temperature', self.temperature),
            'top_p': kwargs.get('top_p', 0.95),
            'top_k': kwargs.get('top_k', 40),
            'repeat_penalty': kwargs.get('repeat_penalty', 1.1),
            'stop': kwargs.get('stop', ["</s>", "\n\n"])
        }
        
        # í…ìŠ¤íŠ¸ ìƒì„±
        response = self.model(
            formatted_prompt,
            **params
        )
        
        return response['choices'][0]['text'].strip()
    
    def generate_stream(self, prompt: str, **kwargs) -> Generator:
        """í…ìŠ¤íŠ¸ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)"""
        if not self.model:
            yield "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            return
        
        formatted_prompt = self.apply_template(prompt)
        
        params = {
            'max_tokens': kwargs.get('max_tokens', self.max_tokens),
            'temperature': kwargs.get('temperature', self.temperature),
            'top_p': kwargs.get('top_p', 0.95),
            'stream': True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
        }
        
        # ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
        stream = self.model(
            formatted_prompt,
            **params
        )
        
        for output in stream:
            chunk = output['choices'][0]['text']
            if chunk:
                yield chunk
    
    def apply_template(self, prompt: str) -> str:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©"""
        # Mistral Instruct í…œí”Œë¦¿
        template = f"""<s>[INST] {prompt} [/INST]"""
        return template
```

### 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê´€ë¦¬
```python
# llm/models.py
from django.db import models

class PromptTemplate(models.Model):
    name = models.CharField(max_length=100)
    system_prompt = models.TextField(
        help_text="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (AIì˜ ì—­í•  ì •ì˜)"
    )
    user_template = models.TextField(
        help_text="ì‚¬ìš©ì ë©”ì‹œì§€ í…œí”Œë¦¿"
    )
    assistant_template = models.TextField(
        help_text="AI ì‘ë‹µ í…œí”Œë¦¿"
    )
    is_active = models.BooleanField(default=False)
    created_at = models.DateTimeField(auto_now_add=True)
    
    # Few-shot ì˜ˆì œ
    examples = models.JSONField(
        default=list,
        help_text="ëŒ€í™” ì˜ˆì œë“¤"
    )
    
    def format_prompt(self, user_message: str) -> str:
        """í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…"""
        prompt = self.system_prompt + "\n\n"
        
        # Few-shot ì˜ˆì œ ì¶”ê°€
        for example in self.examples:
            prompt += f"User: {example['user']}\n"
            prompt += f"Assistant: {example['assistant']}\n\n"
        
        # ì‹¤ì œ ì‚¬ìš©ì ë©”ì‹œì§€
        prompt += f"User: {user_message}\nAssistant:"
        
        return prompt
```

### 4. RAG (Retrieval-Augmented Generation)
```python
# llm/rag_service.py
import chromadb
from chromadb.utils import embedding_functions
from typing import List, Dict

class RAGService:
    def __init__(self):
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(
            path="./rag_vectors.db"
        )
        
        # ì„ë² ë”© í•¨ìˆ˜ (ë¬¸ì¥ì„ ë²¡í„°ë¡œ ë³€í™˜)
        self.embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="all-MiniLM-L6-v2"
        )
        
        # ì»¬ë ‰ì…˜ ìƒì„±/ë¡œë“œ
        self.collection = self.client.get_or_create_collection(
            name="documents",
            embedding_function=self.embedding_fn
        )
    
    def add_document(self, text: str, metadata: Dict = None):
        """ë¬¸ì„œ ì¶”ê°€"""
        doc_id = f"doc_{self.collection.count() + 1}"
        
        self.collection.add(
            documents=[text],
            metadatas=[metadata or {}],
            ids=[doc_id]
        )
        
        return doc_id
    
    def search(self, query: str, n_results: int = 3) -> List[Dict]:
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
        results = self.collection.query(
            query_texts=[query],
            n_results=n_results
        )
        
        documents = []
        for i in range(len(results['documents'][0])):
            documents.append({
                'text': results['documents'][0][i],
                'metadata': results['metadatas'][0][i],
                'distance': results['distances'][0][i]
            })
        
        return documents
    
    def generate_with_context(self, 
                             query: str, 
                             llm_service) -> str:
        """ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì‘ë‹µ ìƒì„±"""
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        relevant_docs = self.search(query, n_results=3)
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join([
            f"ì°¸ê³  ë¬¸ì„œ {i+1}: {doc['text']}"
            for i, doc in enumerate(relevant_docs)
        ])
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¤ìŒ ì°¸ê³  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.

{context}

ì§ˆë¬¸: {query}

ë‹µë³€:"""
        
        # LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        response = llm_service.generate(prompt)
        
        return response, relevant_docs
```

## ğŸ›ï¸ AI ì„¤ì • ê´€ë¦¬

### 1. ëª¨ë¸ íŒŒë¼ë¯¸í„°
```python
# ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ëª…
class GenerationParams:
    temperature: float = 0.7  # ì°½ì˜ì„± (0=ë³´ìˆ˜ì , 1=ì°½ì˜ì )
    max_tokens: int = 512     # ìµœëŒ€ í† í° ìˆ˜
    top_p: float = 0.95      # ëˆ„ì  í™•ë¥  ì„ê³„ê°’
    top_k: int = 40          # ìƒìœ„ Kê°œ í† í°ë§Œ ê³ ë ¤
    repeat_penalty: float = 1.1  # ë°˜ë³µ í˜ë„í‹°
    
    def to_dict(self):
        return {
            'temperature': self.temperature,
            'max_tokens': self.max_tokens,
            'top_p': self.top_p,
            'top_k': self.top_k,
            'repeat_penalty': self.repeat_penalty
        }
```

### 2. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
```python
# ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
PROMPT_TEMPLATES = {
    'default': {
        'system': "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
        'format': "{system}\n\nUser: {user}\nAssistant:"
    },
    
    'coding': {
        'system': "ë‹¹ì‹ ì€ ì „ë¬¸ í”„ë¡œê·¸ë˜ë¨¸ì…ë‹ˆë‹¤. ì½”ë“œë¥¼ ëª…í™•í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.",
        'format': "{system}\n\nìš”êµ¬ì‚¬í•­: {user}\n\nì½”ë“œ:"
    },
    
    'teacher': {
        'system': "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì„ ìƒë‹˜ì…ë‹ˆë‹¤. ë³µì¡í•œ ê°œë…ì„ ì‰½ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.",
        'format': "{system}\n\ní•™ìƒ ì§ˆë¬¸: {user}\n\nì„ ìƒë‹˜ ë‹µë³€:"
    },
    
    'creative': {
        'system': "ë‹¹ì‹ ì€ ì°½ì˜ì ì¸ ì‘ê°€ì…ë‹ˆë‹¤.",
        'format': "{system}\n\nì£¼ì œ: {user}\n\nì°½ì‘ë¬¼:"
    }
}
```

## ğŸ’¾ Fine-tuning (ë¯¸ì„¸ ì¡°ì •)

### 1. ë°ì´í„°ì…‹ ì¤€ë¹„
```python
# í•™ìŠµ ë°ì´í„° í˜•ì‹ (JSONL)
{
  "instruction": "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
  "input": "",
  "output": "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•:\n1. sort() ë©”ì„œë“œ: list.sort()\n2. sorted() í•¨ìˆ˜: sorted_list = sorted(list)"
}
```

### 2. Fine-tuning í”„ë¡œì„¸ìŠ¤
```python
# llm/finetuning_service.py
import json
import subprocess
from pathlib import Path

class FineTuningService:
    def __init__(self):
        self.base_model_path = "models/mistral-7b.gguf"
        self.output_dir = "models/finetuned"
    
    def prepare_dataset(self, raw_data: List[Dict]) -> str:
        """ë°ì´í„°ì…‹ ì¤€ë¹„"""
        dataset_path = "datasets/training_data.jsonl"
        
        with open(dataset_path, 'w', encoding='utf-8') as f:
            for item in raw_data:
                # Alpaca í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                formatted = {
                    "instruction": item.get("instruction", ""),
                    "input": item.get("input", ""),
                    "output": item.get("output", "")
                }
                f.write(json.dumps(formatted, ensure_ascii=False) + '\n')
        
        return dataset_path
    
    def start_training(self, dataset_path: str, **kwargs):
        """í•™ìŠµ ì‹œì‘"""
        config = {
            'learning_rate': kwargs.get('learning_rate', 3e-4),
            'batch_size': kwargs.get('batch_size', 4),
            'num_epochs': kwargs.get('num_epochs', 3),
            'warmup_steps': kwargs.get('warmup_steps', 100),
        }
        
        # LoRA ì„¤ì • (Low-Rank Adaptation)
        lora_config = {
            'r': 8,  # LoRA rank
            'lora_alpha': 16,
            'lora_dropout': 0.05,
            'target_modules': ["q_proj", "v_proj"]
        }
        
        # í•™ìŠµ ëª…ë ¹ì–´ ì‹¤í–‰ (ì˜ˆì‹œ)
        cmd = [
            "python", "train.py",
            "--model_path", self.base_model_path,
            "--data_path", dataset_path,
            "--output_dir", self.output_dir,
            "--learning_rate", str(config['learning_rate']),
            "--batch_size", str(config['batch_size']),
            "--num_epochs", str(config['num_epochs'])
        ]
        
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        
        return process
```

## ğŸ” ëª¨ë¸ ì„±ëŠ¥ ìµœì í™”

### 1. ìºì‹±
```python
from functools import lru_cache
import hashlib

class CachedLLMService(LLMService):
    @lru_cache(maxsize=100)
    def generate_cached(self, prompt_hash: str, **kwargs):
        """ìºì‹œëœ ìƒì„±"""
        return super().generate(prompt_hash, **kwargs)
    
    def generate(self, prompt: str, **kwargs):
        # í”„ë¡¬í”„íŠ¸ í•´ì‹œ ìƒì„±
        prompt_hash = hashlib.md5(
            prompt.encode()
        ).hexdigest()
        
        # ìºì‹œ í™•ì¸
        if kwargs.get('use_cache', True):
            return self.generate_cached(prompt_hash, **kwargs)
        
        return super().generate(prompt, **kwargs)
```

### 2. ë°°ì¹˜ ì²˜ë¦¬
```python
async def batch_generate(self, prompts: List[str]) -> List[str]:
    """ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ ë™ì‹œ ì²˜ë¦¬"""
    import asyncio
    
    tasks = [
        self.generate_async(prompt) 
        for prompt in prompts
    ]
    
    responses = await asyncio.gather(*tasks)
    return responses
```

### 3. GPU ê°€ì†
```python
# GPU ì‚¬ìš© ì„¤ì •
def setup_gpu():
    import torch
    
    if torch.cuda.is_available():
        device = "cuda"
        gpu_layers = 32
    elif torch.backends.mps.is_available():  # Mac M1/M2
        device = "mps"
        gpu_layers = 1
    else:
        device = "cpu"
        gpu_layers = 0
    
    return device, gpu_layers
```

## ğŸ“Š ëª¨ë¸ ëª¨ë‹ˆí„°ë§

### 1. ì„±ëŠ¥ ë©”íŠ¸ë¦­
```python
class ModelMetrics:
    def __init__(self):
        self.total_requests = 0
        self.total_tokens = 0
        self.response_times = []
    
    def log_request(self, 
                   prompt_tokens: int,
                   completion_tokens: int,
                   response_time: float):
        """ìš”ì²­ ë¡œê¹…"""
        self.total_requests += 1
        self.total_tokens += prompt_tokens + completion_tokens
        self.response_times.append(response_time)
    
    def get_stats(self):
        """í†µê³„ ë°˜í™˜"""
        return {
            'total_requests': self.total_requests,
            'total_tokens': self.total_tokens,
            'avg_response_time': sum(self.response_times) / len(self.response_times),
            'tokens_per_second': self.total_tokens / sum(self.response_times)
        }
```

### 2. í’ˆì§ˆ í‰ê°€
```python
def evaluate_response(response: str, expected: str = None):
    """ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
    metrics = {
        'length': len(response),
        'has_code': '```' in response,
        'politeness': any(word in response.lower() 
                         for word in ['ê°ì‚¬', 'ì£„ì†¡', 'ì•ˆë…•']),
        'relevance': 0.0  # ê´€ë ¨ì„± ì ìˆ˜
    }
    
    if expected:
        # ìœ ì‚¬ë„ ê³„ì‚°
        from difflib import SequenceMatcher
        metrics['relevance'] = SequenceMatcher(
            None, response, expected
        ).ratio()
    
    return metrics
```

## ğŸ“š í•™ìŠµ ìë£Œ

### LLM ì´í•´
- [LLM ì…ë¬¸ ê°€ì´ë“œ](https://www.promptingguide.ai/kr)
- [Hugging Face ê°•ì¢Œ](https://huggingface.co/learn/nlp-course/chapter1/1)
- [LangChain ë¬¸ì„œ](https://python.langchain.com/docs/get_started/introduction)

### ëª¨ë¸ í™œìš©
- [llama.cpp ë¬¸ì„œ](https://github.com/ggerganov/llama.cpp)
- [Mistral AI ë¬¸ì„œ](https://docs.mistral.ai/)
- [OpenAI Cookbook](https://cookbook.openai.com/)

### í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
- [í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê°€ì´ë“œ](https://www.promptingguide.ai/kr/introduction/basics)
- [ChatGPT Prompt Engineering](https://platform.openai.com/docs/guides/prompt-engineering)

### Fine-tuning
- [LoRA ë…¼ë¬¸ ì´í•´í•˜ê¸°](https://arxiv.org/abs/2106.09685)
- [QLoRA íŠœí† ë¦¬ì–¼](https://github.com/artidoro/qlora)

## ğŸ’¡ ì‹¤ìŠµ ì•„ì´ë””ì–´

1. **í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë§Œë“¤ê¸°**
   - ë‹¤ì–‘í•œ ì—­í• ì˜ AI ë§Œë“¤ê¸°
   - Few-shot í•™ìŠµ ì‹¤í—˜
   - Chain-of-Thought êµ¬í˜„

2. **RAG ì‹œìŠ¤í…œ êµ¬ì¶•**
   - PDF ë¬¸ì„œ ì„ë² ë”©
   - ê²€ìƒ‰ ì •í™•ë„ ê°œì„ 
   - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰

3. **ëª¨ë¸ ìµœì í™”**
   - ì‘ë‹µ ì†ë„ ê°œì„ 
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¤„ì´ê¸°
   - ë°°ì¹˜ ì²˜ë¦¬ êµ¬í˜„

## ğŸ¤” ìƒê°í•´ë³¼ ë¬¸ì œ

1. AIì˜ í™˜ê°(Hallucination)ì„ ì–´ë–»ê²Œ ì¤„ì¼ê¹Œ?
2. ëª¨ë¸ì˜ í¸í–¥ì„±ì„ ì–´ë–»ê²Œ í•´ê²°í• ê¹Œ?
3. ê°œì¸ì •ë³´ ë³´í˜¸ëŠ” ì–´ë–»ê²Œ í• ê¹Œ?

---

ë‹¤ìŒ ë¬¸ì„œ: [07-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜.md](./07-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜.md) ì—ì„œ ì „ì²´ ì‹œìŠ¤í…œ êµ¬ì¡°ë¥¼ ì‹œê°í™”í•´ë´ìš”!